---
layout: post
title: 机器学习西瓜书 学习笔记
date: 2020-10-26 16:43:42
updated: 2020-12-17 23:15:05
categories: 学习笔记
tags: 
    - 笔记
    - 学习
    - 学习笔记
    - 西瓜书
    - 机器学习
    - 周志华
urlname: 26
comment: true
mathjax: true
---

记录自己的学习，以及为期末考试准备一份复习资料，不过复习的主要目的也不是为了那一份资料，而是复习的过程吧。随后的还有模式识别、计算机组成原理、单片机原理。我个人一直都对机器学习的「无中生有」比较感兴趣，正是这样的兴趣推动了我选择了这个专业方向。目前看来，大量的数学公式推导确实让我非常吃力，但是也算是满足了自己一窥端倪的好奇心。

<!-- more -->

# 写在最前面

本系列文章都会遵守 [中文文案排版指北](https://github.com/sparanoid/chinese-copywriting-guidelines)，愿各位看官和我都能够跟自己爱的人结婚😀。

「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。

与大家共勉之。」——[vinta/paranoid-auto-spacing](https://github.com/vinta/pangu.js)

# 绪论

## 基本术语

假设收集了一批关于西瓜的数据

| 色泽 | 根蒂 | 敲声 |
| :--: | :--: | :--: |
| 青绿 | 蜷缩 | 混响 |
| 乌黑 | 稍蜷 | 沉闷 |
| 浅白 | 硬挺 | 清脆 |

数据集：上面这一组数据的集合。

**样本**：每一条（行）记录是关于一个时间或对象（这里是西瓜）的描述。

**属性/特征**：反映时间或对象在某方面的表现或性质，例如「色泽」、「根蒂」、「敲声」。

属性值：属性上的取值，例如「青绿」、「乌黑」。

**属性空间**/样本空间：属性张成的空间，比如把“色泽” “根蒂” “敲声”作为三个坐标轴，则他们张成一个用于描述西瓜的三维空间。这个空间就是属性空间。

**特征向量**：把每个属性作为一个坐标轴，则它们能够组成一个多维空间，那么每一个实例都能够在空间中找到自己的坐标位置。因为每个点都对应一个坐标向量，因此一个实例也称特征向量。

**维数**：因为每个属性都作为一个坐标轴，而又因为有多少个坐标轴我们就将这个空间叫做几维空间，所以维数也就是样本有多少个属性。

**学习/训练**：从数据中学的模型的过程，学习的过程是通过执行某个学习算法来完成的。

训练数据：训练过程中使用的数据。

**训练样本**：训练过程中每个样本。

**训练集**：训练样本组成的集合。

假设：学得模型对应关于数据的某种潜在的规律。

标记：样本的好或坏，一般都是布尔值。

样例：拥有标记值的样本。

标记空间/输出空间：所有标记的集合。

**分类任务**：预测的结果是布尔值的任务。

**回归任务**：预测的是连续值，比如西瓜的成熟度 0.95、0.37。

二分类任务：只要涉及的类别只有两个，通常一个为正类，另一个为反类。

多分类任务：涉及多个类别，也即是大于两个吧。

**测试**：使用学得的模型进行预测。

**测试样本**：被预测的样本。

聚类：将训练集中的西瓜分成若干组，例如高工资、低工资。

簇：被聚类成分成的组，每一组就是一个簇。

**监督学习**：有标记信息，分类任务和回归任务。

**半监督学习**：监督学习与无监督学习相结合的一种学习方法。半监督学习使用大量的未标记数据，以及同时使用标记数据，来进行模式识别工作。 

**无监督学习**：无标记信息，聚类任务。

**泛化能力**：学的模型适用于新样本的能力。具有强泛化能力的模型能很好地适应于整个样本空间。

独立同分布：可以先假设样本空间都服从一个未知的分布，我们获得的每个样本都是独立的从这个分布上采样获得的，即独立同分布。一般而言，训练样本越多，我们获得关于该分布的信息越多，就越有可能通过学习获得具有强泛化能力的模型。

### 术语例题

| 编号 | 姓名 | 年收入 | 性别 | 职业 | 好顾客 |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 1 | 张三 | 高 | 男 | 程序员 | 是 |
| 2 | 李四 | 高 | 男 | 企业家 | 是 |
| 3 | 王五 | 中 | 男 | 公务员 | 否 |
| 4 | 周六 | 低 | 女 | 学生 | 否 |
| 5 | 钱七 | 中 | 女 | 教师 | 否 |

- 上表中样本是？
- 一共有多少样本？
- 样本的属性都是？
- 样本标记是？
- 用户「周六」属性值和标记值是？

{% tabs Solution one, -1 %}
<!-- tab 答案 @key -->
- 样本是顾客。
- 一共 5 个。
- 年收入、性别和职业。
- 是否为顾客（或好顾客）。
- 属性值&标记值：年收入=低、性别=女和职业=学生。
<!-- endtab -->
{% endtabs %}

## 假设空间

科学推理的两个基本手段：演绎、归纳。

演绎：从一般到特殊的特化，即从基础原理推演出具体状况，例如数学公式就是从一些常识和推理逻辑，推导出了相洽的定理。

归纳：从特殊到一般的泛化过程，就是从具体的事实归结出一般性规律。机器学习也就是用的这种推理方法。

广义的归纳学习：从样例中学习。

狭义的归纳学习：从训练数据中学得概念。

狭义的归纳学习，有时候也成为概念学习。概念学习中最基础的就是布尔概念学习，也就是对「是」「不是」这样的目标概念的学习。

### 假设的确定

我们可以把学习的过程看做一个在所有假设组成的空间中进行搜索的过程，搜索的目标是 找到与训练集匹配的假设，也就是能够将训练集中的瓜判断正确的假设。

假设的表示一旦确定下来，假设空间及其规模大小就确定了。

> 简单的讲，假设就是所有属性的所有取值情况（不受数据集约束）。

| 编号 | 色泽 | 根蒂 | 敲声 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: |
| 1 | 青绿 | 蜷缩 | 浊响 | 是 |
| 2 | 乌黑 | 蜷缩 | 浊响 | 是 |
| 3 | 青绿 | 硬挺 | 清脆 | 否 |
| 4 | 乌黑 | 稍蜷 | 沉闷 | 否 |

与数据集匹配的假设就是

色泽 = *; 根蒂 = 蜷缩； 敲声 = 浊响

也就是好瓜就是，根蒂蜷缩、敲声浊响，什么色泽都行的瓜。

这个能够判断所有数据集的“假设集合”已经非常接近版本空间，版本空间与这个“假设集合”的区别主要是因为。**版本空间是将所有假设全部列出来后，然后用数据集一个一个去进行搜索，然后将删除与正例不一致的假设**。这里主要的问题就是，对于一些属性它无法进行判断，因为数据集没有覆盖完整。

在做题的时候，也只有尽可能的往上面再想一层，看看它那个属性的上一层就算取任意值也算正例。

### 版本空间例题1

| 编号 | 姓名 | 年收入 | 性别 | 职业 | 好顾客 |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 1 | 张三 | 高 | 男 | 程序员 | 是 |
| 2 | 李四 | 高 | 男 | 企业家 | 是 |
| 3 | 王五 | 中 | 男 | 公务员 | 否 |
| 4 | 周六 | 低 | 女 | 学生 | 否 |
| 5 | 钱七 | 中 | 女 | 教师 | 否 |

给出上表所对应的版本空间，用符号 * 表示取任何值都可以。

{% tabs Solution tow, -1 %}
<!-- tab 答案 @key -->

![1](https://st.blackyau.net/blog/26/1.png)

<!-- endtab -->
{% endtabs %}

### 版本空间例题2

| 编号 | 色泽 | 根蒂 | 敲声 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: |
| 1 | 青绿 | 蜷缩 | 浊响 | 是 |
| 2 | 乌黑 | 蜷缩 | 浊响 | 是 |
| 3 | 青绿 | 硬挺 | 清脆 | 否 |
| 4 | 乌黑 | 稍蜷 | 沉闷 | 否 |

该数据集的版本空间是？

{% tabs Solution three, -1 %}
<!-- tab 答案 @key -->

![2](https://st.blackyau.net/blog/26/2.svg)

<!-- endtab -->
{% endtabs %}

# 归纳偏好

通过上面的 版本空间例题2 可以了解到，我们通过学习获得了 3 个与训练集一致的假设（版本空间里面有 3 种不同的假设）。但是有一个问题，就是它在面临新样本的时候，可能会输出不同的结果。

比如这里有个新瓜

色泽 = 青绿； 根茎 = 蜷缩； 敲声 = 沉闷

如果使用 B 条件（点开上面的答案）判断，那么它会把这个新瓜判断为**好瓜**。

但是如果使用 C 条件判断，那么它会把这个新瓜判断为**坏瓜**。

如果在我们使用机器学习算法的时候，遇到这种样本它每次都随机挑选一个条件进行判断，进而导致每次预测的时候结果时而好又时而坏，这样的学习结果显然是没有意义的。

所以我们需要让机器学习算法在学习的过程中，要对于某种类型的假设有偏好。我们就称其为**归纳偏好**，或简称为偏好。

比如它的偏好如果是“尽可能特殊”的模型，那么它就会选择版本空间右上角那一个，因为这个模型确定了 3 种属性分别都是一个具体的取值。

又比如它的偏好是“尽可能一般”的模型，并且由于某种原因它更相信根蒂，那么它就会选择版本空间左上角那一个。因为这个模型更多的使用了 * 来确认取值，同时它指定了根茎是蜷缩。

## 奥卡姆剃刀

奥卡姆剃刀就是一种用来，评判哪一种“偏好”更好的原则。也即是*若有多个假设与观察一致，则选择最简单的那个*。

> 可以总结为，简单的就是最好的

这里的每个训练样本是图中的一个点（x，y），要学习一个与训练集一致的模型，相当于找到一条穿过所有训练样本的曲线。

![存在多条曲线与有限样本训练集一致](https://st.blackyau.net/blog/26/3.png)

在这里我们就列举出了两条曲线，其中 A 是较为平滑的一条它的方程式是 $y=-x^2+6x+1$ ，而 B 曲线是要复杂很多。

假设我们认为“更平滑”意味着“更简单”，所以在上面的图中我们会自然的偏好于选择更平滑的曲线 A。

但是奥卡姆剃刀并不是唯一可行的原则，因为在很多问题中，我们并不能找到**哪一种假设更简单**。同时也有一个问题，万一所有的样本都比较刁钻，他们都正好和 B 完全重合了呢？这个也正是下面这个定理要说明的。

## 没有免费的午餐

首先你不要被这个定理的名字所迷惑，可以先把这个命名放在另一边，看看定理的内容。

> 简单的描述这个定理就是，没有任何一个机器学习算法适合所有情况。

这个定理也很符合我们的直觉，就和上面奥卡姆剃刀举例的那两个曲线一样。A 曲线所代表的的机器学习算法，比 B 曲线的机器学习算法要牛逼些吗？并不是，因为 没有免费的午餐 定理证明了他们两种及其学习算法的正确率是完全一致的。

说人话就是，对于所有机器学习问题，任何一种算法的期望性能都是相等的。

但是也不要因为这个定理就觉得机器学习没意思了，因为这个定理有一个前提。也就是，所有问题出现的机会相同、或所有问题同等重要。但是在实际情况中，我们的很多问题都不是这样的，你只需要关注自己需要解决的问题就行了。

比如在学校里面有个电瓶车代步是一个很方便的选择，但是你如果要去外地又或是去国外，那么你可能坐飞机会更好一些。但是你就是想在学校里面找个代步工具，那么买飞机的事情你就不需要关心了。

返回到这个定理上面来，其实它想说的和丢了芝麻捡西瓜一样，也就是**有得必有失**。某一个机器学习算法在某一个领域非常好用，但是换到另一个地方却完全不好使了。

也就是说，当天上突然掉馅饼的时候你现在吃到了，但是你之后就会遇到很倒霉的事情。这么说应该就能理解没有免费的午餐是什么意思了。

# 术语中英对照

| 中文 | 英文 | 备注 |
| :--: | :--: | :--: |
| 数据集 | data set | |
| 实例 | instance | 每一条记录 |
| 样本 | sample | 又名实例 |
| 属性 | attribute | 描述样本的什么方面 |
| 特征 | feature | 又名属性 |
| 属性样本 | attribute space | |
| 特征向量 | feature vector | 在空间中确认一个点 |
| 维数 | dimensionality | 样本有多少属性 |
| 学习 | learning |  |
| 训练数据 | training data | |
| 训练样本 | training sample | |
| 训练集 | training set | |
| 假设 | hypothesis | 关于数据的某种规律 |
| 标记 | label | 真假 |
| 样例 | example | 有标记的样本 |
| 标记空间 | label space | 所有标记的集合 |
| 分类 | classification | 预测的布尔值 |
| 回归 | regression | 预测的连续值 |
| 二分类 | binary classification | 预测类别只有两个 |
| 多分类 | multi-class classification | 预测类别有两个以上 |
| 测试 | testing | |
| 测试样本 | testing sample | |
| 聚类 | clustering | 给训练集分组 |
| 簇 | cluster | 每个组就是一个簇 |
| 监督学习 | supervised learning | 有标记 |
| 无监督学习 | unsupervised learning | 无标记 |
| 泛化 | generalization | 学习样本的能力 |
| 归纳 | induction | 从具体事实归纳出规律 |
| 演绎 | deduction | 从基础原理推导出具体情况 |
| 特化 | specialization | 描述演绎的过程 |
| 归纳学习 | inductive learning | 从具体事实归结出一般性规律 |
| 概念 | concept | 对样本的描述 |

# 参考

[奥卡姆剃刀和没有免费的午餐定理@LogM's Blog](https://imlogm.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/occam-razor-NFL/)


